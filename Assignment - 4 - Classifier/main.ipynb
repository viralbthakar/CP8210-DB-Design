{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 - Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Name`** Montgomery Gole, Viral Bankimbhai Thakar\n",
    "\n",
    "**`Email`** mgole@torontomu.ca, vthakar@torontomu.ca\n",
    "\n",
    "**`Student ID`** 501156495, 501213983"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of AND Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An AND function or AND gate receives two inputs $(x_1, x_2)$ and returns an output as following table:\n",
    "    \n",
    "| $x_1$ | $x_2$ | $out$ |\n",
    "| - | - | --- |\n",
    "| 0 | 0 | 0 |\n",
    "| 0 | 1 | 0 |\n",
    "| 1 | 0 | 0 |\n",
    "| 1 | 1 | 1 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing the AND function. \n",
    "x1 = [1,1,0,0] \n",
    "x2 = [1,0,1,0]\n",
    "plt.scatter(x1, x2)\n",
    "plt.scatter([x1[0]],[x2[0]])\n",
    "plt.title(\"AND function on inputs x1 and x2\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first let's try to understand the requirements for the neural network model and solve the problem without explicit programming or code."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a `heavside step function` $H(x)$ such that \n",
    "\n",
    "$$\n",
    "        H(x) = \\begin{cases}\n",
    "                        0, & \\text{if } x < c \\\\\n",
    "                        1, & \\text{if } x \\geq c\n",
    "                \\end{cases}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we let $c = 0.5$, we can begin to see how this activation function can allow single neuron (perceptron) to estimate the AND function of two inputs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider our AND function as, \n",
    "$$f(x_1, x_2) = H(w_1*x_1 + w_2*x_2 + b, 0.5)$$ \n",
    "where,\n",
    "- $w_1*x_1 + w_2*x_2 + b$ indicates the weighted linear combination of inputs $x_1$ and $x_2$.\n",
    "- $x_1$ and $x_2$ indicates the inputs.\n",
    "- $w_1$ and $w_2$ are the weights associated with each inputs.\n",
    "- $b$ is a bias tear.\n",
    "- $H(\\cdot)$ is activation function or heiavside step function. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we solve for $w_1$, $w_2$, and $b$ with respect to the following ... \n",
    "\n",
    "$$  \n",
    "                1 = H(w_1*1 + w_2*1 + b, 0.5) \\\\\n",
    "                0 = H(w_1*0 + w_2*1 + b, 0.5) \\\\\n",
    "                0 = H(w_1*1 + w_2*0 + b, 0.5) \\\\\n",
    "                0 = H(w_1*0 + w_2*0 + b, 0.5) \\\\\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then simplifying above equations ...\n",
    "$$\n",
    "    1 = H(w_1 + w_2 + b, 0.5) \\\\\n",
    "    0 = H(w_2 + b, 0.5) \\\\\n",
    "    0 = H(w_1 + b, 0.5) \\\\\n",
    "    0 = H(b, 0.5)\n",
    "$$\n",
    "\n",
    "It is clear that a solution to the system of equations has $w_1 = 0.25$, $w_2 = 0.25$, and $b < 0.25$. Also, it is clear that only a single neuron is necessary to create an AND gate. Let's create a simple AND function using these findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 AND 1 = 1\n",
      "1 AND 0 = 0\n",
      "0 AND 1 = 0\n",
      "0 AND 0 = 0\n"
     ]
    }
   ],
   "source": [
    "def step_fn(out, c):\n",
    "    return 0 if out < c else 1\n",
    "\n",
    "def and_fn(x1, x2):\n",
    "    y = (0.25*x1) + (0.25*x2)\n",
    "    return step_fn(y, 0.5)\n",
    "    \n",
    "X = [[1,1], [1,0], [0,1], [0,0]] \n",
    "y = [1, 0, 0, 0]\n",
    "for inp in X:\n",
    "    print(f\"{inp[0]} AND {inp[1]} = {and_fn(inp[0], inp[1])}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see whether our neural network can learn the same set of parameters or not!!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A perceptron is a linear function that finds some hyperplane to separate data into two classes.\n",
    "\n",
    "In this case *hyperplane* is referring to the linear boundary which splits data into two categories represented by the line orthagonal to $t = x_1*w_1 + x_2*w_2 + b$ where $t$ is the threshold for the step function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe00165e1f0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the input data and expected output for the AND function\n",
    "input_data = tf.constant([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=tf.float32)\n",
    "expected_output = tf.constant([[0], [0], [0], [1]], dtype=tf.float32)\n",
    "\n",
    "# Define the neural network architecture\n",
    "input_layer = tf.keras.layers.Input(shape=(2,))\n",
    "output_layer = tf.keras.layers.Dense(1, activation='sigmoid', name=\"perceptron\")(input_layer)\n",
    "# output_layer = tf.keras.layers.Dense(1, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "# Define the model using the input and output layers\n",
    "model = tf.keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile the model with binary cross-entropy loss and Adam optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Train the model for 500 epochs\n",
    "model.fit(input_data, expected_output, epochs=500, verbose=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our model by predicting for different inputs ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input point: [0. 0.], Expected output point: [0.], Predicted output point: [[0.]]\n",
      "Input point: [0. 1.], Expected output point: [0.], Predicted output point: [[0.]]\n",
      "Input point: [1. 0.], Expected output point: [0.], Predicted output point: [[0.]]\n",
      "Input point: [1. 1.], Expected output point: [1.], Predicted output point: [[1.]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(input_data)):\n",
    "    input_point = input_data[i]\n",
    "    expected_output_point = expected_output[i]\n",
    "    predicted_output_point = model.predict(tf.reshape(input_point, (1, 2)), verbose=0).round()\n",
    "    print(f\"Input point: {input_point}, Expected output point: {expected_output_point}, Predicted output point: {predicted_output_point}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the weights of our model ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.8726175]\n",
      " [1.8589239]]\n"
     ]
    }
   ],
   "source": [
    "weights = model.layers[1].get_weights()[0]\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.89759]\n"
     ]
    }
   ],
   "source": [
    "biases = model.layers[1].get_weights()[1]\n",
    "print(biases)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see that our model's weights and bias tearms are aligning with our manual calculation of weights and biases. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of OR Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will directly try our Neural Network model for OR function. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An OR function or AND gate receives two inputs $(x_1, x_2)$ and returns an output as following table:\n",
    "    \n",
    "| $x_1$ | $x_2$ | $out$ |\n",
    "| - | - | --- |\n",
    "| 0 | 0 | 0 |\n",
    "| 0 | 1 | 1 |\n",
    "| 1 | 0 | 1 |\n",
    "| 1 | 1 | 1 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdfd0efceb0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the input data and expected output for the OR function\n",
    "input_data = tf.constant([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=tf.float32)\n",
    "expected_output = tf.constant([[0], [1], [1], [1]], dtype=tf.float32)\n",
    "\n",
    "# Define the neural network architecture with a single neuron and the sigmoid activation function\n",
    "input_layer = tf.keras.layers.Input(shape=(2,))\n",
    "output_layer = tf.keras.layers.Dense(1, activation='sigmoid', name=\"perceptron\")(input_layer)\n",
    "\n",
    "# Define the model using the input and output layers\n",
    "model = tf.keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile the model with binary cross-entropy loss and Adam optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Train the model for 500 epochs\n",
    "model.fit(input_data, expected_output, epochs=500, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our model by predicting for different inputs ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input point: [0. 0.], Expected output point: [0.], Predicted output point: [[0.]]\n",
      "Input point: [0. 1.], Expected output point: [1.], Predicted output point: [[1.]]\n",
      "Input point: [1. 0.], Expected output point: [1.], Predicted output point: [[1.]]\n",
      "Input point: [1. 1.], Expected output point: [1.], Predicted output point: [[1.]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(input_data)):\n",
    "    input_point = input_data[i]\n",
    "    expected_output_point = expected_output[i]\n",
    "    predicted_output_point = model.predict(tf.reshape(input_point, (1, 2)), verbose=0).round()\n",
    "    print(f\"Input point: {input_point}, Expected output point: {expected_output_point}, Predicted output point: {predicted_output_point}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the weights of our model ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.5497346]\n",
      " [1.7687374]]\n"
     ]
    }
   ],
   "source": [
    "weights = model.layers[1].get_weights()[0]\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02915679]\n"
     ]
    }
   ],
   "source": [
    "biases = model.layers[1].get_weights()[1]\n",
    "print(biases)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. In SVM what is the meaning of margin? Which are the equations of the two margin hyperplanes H+ and H-?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have mentioned in the first question, the perceptron finds a hyperplane which separates the two classes in the AND/OR function. \n",
    "\n",
    "However, there are infinite hyperplanes which could separate the two classes. `The objective of support vector machine (SVM) is to find which of these hyperplanes is optimal.` This hyperplane is also called `decision boundry`.\n",
    "\n",
    "SVM aims to find the hyperplane or decision boundary such that the distance between the decision boundary (or hyperplane) and the closest data points from either classes is maximum.\n",
    "\n",
    "SVM uses the term `\"margin\"` to describe the distance between the decision boundary (or hyperplane) and the closest data points for each class. The classifier can generalize to unseen data more effectively as the margin gets larger. The decision boundary is determined by these closest data points, which are called support vectors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two margin hyperplanes in SVM: the positive margin hyperplane (H+) and the negative margin hyperplane (H-). The positive margin hyperplane (H+) is the hyperplane that is parallel to the decision boundary and passes through the closest positive support vector. Similarly, the negative margin hyperplane (H-) is the hyperplane that is parallel to the decision boundary and passes through the closest negative support vector.\n",
    "\n",
    "The equations of the two margin hyperplanes can be derived as follows:\n",
    "\n",
    "$$\n",
    "    H+ : w * x + b = 1 \\\\\n",
    "    H- : w * x + b = -1\n",
    "$$\n",
    "where \n",
    "- $w$ is the weight vector\n",
    "- $x$ is the input vector\n",
    "- $b$ is the bias term "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Consider the three linearly separable two-dimensional input vectors in the following figure. Find the linear SVM that optimally separates the classes by maximizing the margin."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ![image info](./data/q2-2-image.png) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w :  [[1.99804688 2.        ]]\n",
      "b :  [-3.9983724]\n",
      "Support Vectors :  [[0.5 1. ]\n",
      " [1.  0.5]\n",
      " [1.  1.5]]\n",
      "Coefficients of the support vector :  [[-3.99609375e+00 -1.95312500e-03  3.99804688e+00]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0.5, 1], [1, 0.5], [1, 1.5]])\n",
    "y = np.array([-1, -1, 1])\n",
    "clf = SVC(C = 1e5, kernel = 'linear')\n",
    "clf.fit(X, y) \n",
    "\n",
    "print('w : ',clf.coef_)\n",
    "print('b : ',clf.intercept_)\n",
    "print('Support Vectors : ', clf.support_vectors_)\n",
    "print('Coefficients of the support vector : ', clf.dual_coef_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. What is Kernel Function "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, a kernel function is a mathematical function that takes two input vectors in their original feature space and calculates the dot product of these vectors in a transformed feature space. This allows the algorithm to implicitly map the original input data into a higher-dimensional feature space where it is easier to classify using linear algorithms.\n",
    "\n",
    "Kernel functions are commonly used in kernel methods, such as support vector machines (SVMs), to perform nonlinear classification, regression, and other machine learning tasks. By applying a kernel function, SVMs can learn nonlinear decision boundaries in the transformed feature space without having to explicitly compute the mapping to the higher-dimensional space.\n",
    "\n",
    "There are several types of kernel functions, such as linear, polynomial, radial basis function (RBF), and sigmoid, each with its own set of hyperparameters. The choice of kernel function depends on the nature of the data and the task at hand. The RBF kernel is a popular choice due to its ability to capture complex nonlinear relationships between the data points, while the linear kernel is useful when the data is already separable in the original feature space."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Neural Network and SVM in Classification of heart disease data set in R or Python language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOME OF OUR CODE PIECES ARE SAME AS OUR ASSIGNMENT 2 SOLUTION TO AVOID DUPLICATE CODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils import styled_print, download_data, read_and_clean_data, \\\n",
    "     plot_box_plot_hist_plot, plot_count_plot, discrete_to_target_plot, \\\n",
    "     continuous_to_target_plot, correlation_analysis, traditional_feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleveland_url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    0: \"age\",\n",
    "    1: \"sex\",\n",
    "    2: \"cp\",\n",
    "    3: \"trestbps\",\n",
    "    4: \"chol\",\n",
    "    5: \"fbs\",\n",
    "    6: \"restecg\",\n",
    "    7: \"thalach\",\n",
    "    8: \"exang\",\n",
    "    9: \"oldpeak\",\n",
    "    10: \"slope\",\n",
    "    11: \"ca\",\n",
    "    12: \"thal\",\n",
    "    13: \"target\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m› \u001b[4mHeart Disease Data Analysis\u001b[0m\n",
      "    Extracting Data From http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\n"
     ]
    }
   ],
   "source": [
    "styled_print(f\"Heart Disease Data Analysis\", header=True)\n",
    "styled_print(f\"Extracting Data From {cleveland_url}\")\n",
    "cleveland_file = download_data(cleveland_url, path_to_download=\"./data\")\n",
    "cleveland_df = read_and_clean_data(cleveland_file, header=headers.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m› \u001b[4mCleveland Dataframe Info\u001b[0m\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 303 entries, 0 to 302\n",
      "Data columns (total 14 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   age       303 non-null    float64\n",
      " 1   sex       303 non-null    float64\n",
      " 2   cp        303 non-null    float64\n",
      " 3   trestbps  303 non-null    float64\n",
      " 4   chol      303 non-null    float64\n",
      " 5   fbs       303 non-null    float64\n",
      " 6   restecg   303 non-null    float64\n",
      " 7   thalach   303 non-null    float64\n",
      " 8   exang     303 non-null    float64\n",
      " 9   oldpeak   303 non-null    float64\n",
      " 10  slope     303 non-null    float64\n",
      " 11  ca        299 non-null    float64\n",
      " 12  thal      301 non-null    float64\n",
      " 13  target    303 non-null    int64  \n",
      "dtypes: float64(13), int64(1)\n",
      "memory usage: 33.3 KB\n"
     ]
    }
   ],
   "source": [
    "styled_print(f\"Cleveland Dataframe Info\", header=True)\n",
    "cleveland_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Understanding and Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some observations from the `heart-disease.names` file regarding the features.\n",
    "\n",
    "1. `age` is a `continuous` feature which indicates the age of the person in years. \n",
    "2. `sex` is a `binary categorical` feature indicating sex information.\n",
    "    - 1 : male\n",
    "    - 0 : female\n",
    "3. `cp` is a `categorical` feature which indicates the type of chest pain.\n",
    "    - Value 1: typical angina\n",
    "    - Value 2: atypical angina\n",
    "    - Value 3: non-anginal pain\n",
    "    - Value 4: asymptomatic\n",
    "4. `trestbps` is a `continuous` feature indicating resting blood pressure (in mm Hg on admission to the hospital).\n",
    "5. `chol` is a `continuous` feature indicating serum cholestoral in mg/dl.\n",
    "6. `fbs` is a `binary categorical` feature indicating fasting blood sugar > 120 mg/dl.\n",
    "    - 1 : true\n",
    "    - 0 : false\n",
    "7. `restecg` is a `categorical` feature indicating resting electrocardiographic results.\n",
    "    - Value 0: normal\n",
    "    - Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)\n",
    "    - Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n",
    "8. `thalach` is a `continuous` feature indicating maximum heart rate achieved.\n",
    "9. `exang` is a `binary categorical` feature indicating exercise induced angina.\n",
    "    - 1 : yes\n",
    "    - 0 : no\n",
    "10. `oldpeak` is a `continuos` feature indicating ST depression induced by exercise relative to rest.\n",
    "11. `slope` is a `categorical`feature indicating the slope of the peak exercise ST segment.\n",
    "    - Value 1: upsloping\n",
    "    - Value 2: flat\n",
    "    - Value 3: downsloping\n",
    "12. `ca` is a `categorical` feature indicating number of major vessels (0-3) colored by flourosopy.\n",
    "13. `thal` is a `categorical` feature.\n",
    "    - 3 : normal\n",
    "    - 6 : fixed defect\n",
    "    - 7 : reversable defect\n",
    "14. `target` is a `categorical` feature (target) indicating the diagnosis of heart disease (angiographic disease status)\n",
    "\n",
    "**Two main observations:** \n",
    "1. As all of over categorical features are already numerically encoded we will treat them as discrete feature and not traditional categorical features. \n",
    "2. As provided in `heart-disease.names` file:\n",
    "\n",
    "    ```The \"goal\" field refers to the presence of heart disease in the patient.  It is integer valued from 0 (no presence) to 4. Experiments with the Cleveland database have concentrated on simply attempting to distinguish presence (values 1,2,3,4) from absence (value 0).```\n",
    "\n",
    "    So Initially We can convert the `target` into two categories \n",
    "    - 0: Absence of Heart disease\n",
    "    - 1: Presence of Heart disease (Combine current categories 1, 2, 3, and 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\"cp\", \"restecg\", \"slope\", \"thal\", \"ca\"]\n",
    "binary_columns = [\"sex\", \"fbs\", \"exang\"]\n",
    "\n",
    "continuous_columns = [\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\"]\n",
    "discrete_columns = categorical_columns + binary_columns\n",
    "target_column = [\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Copy of Dataframe for Data Processing\n",
    "data_df = cleveland_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing and Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocessing Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    164\n",
       "1     55\n",
       "2     36\n",
       "3     35\n",
       "4     13\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check unique values for target and its percentage\n",
    "data_df[\"target\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping target 2, 3, and 4 to 1. \n",
    "target_mapping = {2: 1, 3: 1, 4: 1}\n",
    "data_df[\"target\"] = data_df[\"target\"].apply(lambda x: 1 if x == 2 or x == 3 or x == 4 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    164\n",
       "1    139\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check unique values for target and its percentage\n",
    "data_df[\"target\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Splitting The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To split the data we are using `train_test_split()` method from `sklearn's` `model_selection` module. The splitting is based on the following parameters:\n",
    "1. `test_size` is set to `0.2`. It will makes sure that we have 20% of our data for testing and rest 80% of data we can use for training and/or cross-validation.\n",
    "2. `random_state` is set to `10`. We can set it to any fix number as it will help us in reproducibility of our experiment.\n",
    "3. `stratify` is set to `target` feature. This will ensure the stratified sampling process. In simple words it will make sure that the distribution of Heart Disease and Non-Heart Disease patient remains as it is even after the split. Refer [this](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-iterators-with-stratification-based-on-class-labels) for further details. \n",
    "4. `shuffle` is set to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(data_df, test_size=.2, random_state=10, stratify=data_df[\"target\"], shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how stratify sampling make sure that the distribution of data is balance after the split too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    54.125413\n",
       "1    45.874587\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check unique values for target and its percentage\n",
    "data_df[\"target\"].value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    54.132231\n",
       "1    45.867769\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check unique values for target and its percentage\n",
    "train_df[\"target\"].value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    54.098361\n",
       "1    45.901639\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check unique values for target and its percentage\n",
    "test_df[\"target\"].value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see that in both training and testing dataset, `54%` of data comes from the `label 0` i.e. Absence of Heart Disease while `46%` of data comes from the `label 1` i.e. Presence of Heart Disease. **These percentages matches the percentage distribution in original dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m› \u001b[4mThere are 242 data points for training and 61 data points for testing.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "styled_print(f\"There are {train_df.shape[0]} data points for training and {test_df.shape[0]} data points for testing.\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why are we splitting data first before any exploratory data analysis or even treating missing values??**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our reasoning to split the data at the very beginning of workflow is to make sure that we can ensure that there is no data leak issues.\n",
    "For example, we usually use median value to replace the missing values in a continuous feature. We want to make sure that the median value which we calculate comes only from the training set and we apply it to test set. This way we can gurantee that even in data preprocessing we are not introducing any direct or indirect data leak issues. \n",
    "\n",
    "This fact is usually ignored in many books and material but in practice it is heavily been used. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Missing Value Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age         0\n",
       "sex         0\n",
       "cp          0\n",
       "trestbps    0\n",
       "chol        0\n",
       "fbs         0\n",
       "restecg     0\n",
       "thalach     0\n",
       "exang       0\n",
       "oldpeak     0\n",
       "slope       0\n",
       "ca          3\n",
       "thal        2\n",
       "target      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age         0\n",
       "sex         0\n",
       "cp          0\n",
       "trestbps    0\n",
       "chol        0\n",
       "fbs         0\n",
       "restecg     0\n",
       "thalach     0\n",
       "exang       0\n",
       "oldpeak     0\n",
       "slope       0\n",
       "ca          1\n",
       "thal        0\n",
       "target      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have a very small number of missing values in the training and test dataset, it would be better to drop those rows instead of trying to figure out strategy to replace them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.dropna()\n",
    "test_df = test_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that all the rows with missing values are dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age         0\n",
       "sex         0\n",
       "cp          0\n",
       "trestbps    0\n",
       "chol        0\n",
       "fbs         0\n",
       "restecg     0\n",
       "thalach     0\n",
       "exang       0\n",
       "oldpeak     0\n",
       "slope       0\n",
       "ca          0\n",
       "thal        0\n",
       "target      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age         0\n",
       "sex         0\n",
       "cp          0\n",
       "trestbps    0\n",
       "chol        0\n",
       "fbs         0\n",
       "restecg     0\n",
       "thalach     0\n",
       "exang       0\n",
       "oldpeak     0\n",
       "slope       0\n",
       "ca          0\n",
       "thal        0\n",
       "target      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    There are 237 data points for training and 60 data points for testing.\n"
     ]
    }
   ],
   "source": [
    "styled_print(f\"There are {train_df.shape[0]} data points for training and {test_df.shape[0]} data points for testing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Creation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will follow these steps to create the `BASELINE` model:\n",
    "- Prepare the data for modeling.\n",
    "    - Create X and Y - (x_train, y_train) and (x_test, y_test)\n",
    "    - Scale Continuous Features using Min-Max Scaler.\n",
    "    - Scale Discrete Features using Min-Max Scaler.\n",
    "- Build the `BASELINE` model on the train data.\n",
    "    - Create `Neural Network` Model\n",
    "- Build the `Experiment` model on the train data.\n",
    "    - Create `SVM` Model\n",
    "- Test the model on the test set.\n",
    "    - Calculate $MAE$ score to measure the performance of the model.\n",
    "    - Calculate $Confusion \\ Matrix$.\n",
    "    - Calculate $Classification \\ Report$ to get Precision, Recall and F1 Score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare the data for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df[target_column[0]].copy()\n",
    "x_train = train_df.drop(target_column[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test_df[target_column[0]].copy()\n",
    "x_test = test_df.drop(target_column[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_copy = x_train.copy(deep=True).reset_index(drop=True)\n",
    "x_test_copy = x_test.copy(deep=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = pd.DataFrame(scaler.transform(x_train),columns = x_train.columns)\n",
    "x_test = pd.DataFrame(scaler.transform(x_test),columns = x_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>407.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>243.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>63.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>271.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>57.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>274.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>305.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>65.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>417.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>43.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>61.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  sex   cp  trestbps   chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "0  63.0  0.0  4.0     150.0  407.0  0.0      2.0    154.0    0.0      4.0   \n",
       "1  48.0  1.0  4.0     130.0  256.0  1.0      2.0    150.0    1.0      0.0   \n",
       "2  50.0  1.0  4.0     150.0  243.0  0.0      2.0    128.0    0.0      2.6   \n",
       "3  63.0  0.0  2.0     140.0  195.0  0.0      0.0    179.0    0.0      0.0   \n",
       "4  49.0  0.0  2.0     134.0  271.0  0.0      0.0    162.0    0.0      0.0   \n",
       "5  57.0  1.0  4.0     152.0  274.0  0.0      0.0     88.0    1.0      1.2   \n",
       "6  60.0  0.0  4.0     158.0  305.0  0.0      2.0    161.0    0.0      0.0   \n",
       "7  65.0  0.0  3.0     140.0  417.0  1.0      2.0    157.0    0.0      0.8   \n",
       "8  43.0  0.0  3.0     122.0  213.0  0.0      0.0    165.0    0.0      0.2   \n",
       "9  61.0  1.0  4.0     138.0  166.0  0.0      2.0    125.0    1.0      3.6   \n",
       "\n",
       "   slope   ca  thal  \n",
       "0    2.0  3.0   7.0  \n",
       "1    1.0  2.0   7.0  \n",
       "2    2.0  0.0   7.0  \n",
       "3    1.0  2.0   3.0  \n",
       "4    2.0  0.0   3.0  \n",
       "5    2.0  1.0   7.0  \n",
       "6    1.0  0.0   3.0  \n",
       "7    1.0  1.0   3.0  \n",
       "8    2.0  0.0   3.0  \n",
       "9    2.0  1.0   3.0  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_copy.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.528302</td>\n",
       "      <td>0.641553</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.633588</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.395833</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.339623</td>\n",
       "      <td>0.296804</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.603053</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.437500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.528302</td>\n",
       "      <td>0.267123</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.435115</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.433962</td>\n",
       "      <td>0.157534</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.824427</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.377358</td>\n",
       "      <td>0.331050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.694656</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.583333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.547170</td>\n",
       "      <td>0.337900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.129771</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.645833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.603774</td>\n",
       "      <td>0.408676</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.687023</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.433962</td>\n",
       "      <td>0.664384</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.656489</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.264151</td>\n",
       "      <td>0.198630</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.717557</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.415094</td>\n",
       "      <td>0.091324</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.412214</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age  sex        cp  trestbps      chol  fbs  restecg   thalach  exang  \\\n",
       "0  0.708333  0.0  1.000000  0.528302  0.641553  0.0      1.0  0.633588    0.0   \n",
       "1  0.395833  1.0  1.000000  0.339623  0.296804  1.0      1.0  0.603053    1.0   \n",
       "2  0.437500  1.0  1.000000  0.528302  0.267123  0.0      1.0  0.435115    0.0   \n",
       "3  0.708333  0.0  0.333333  0.433962  0.157534  0.0      0.0  0.824427    0.0   \n",
       "4  0.416667  0.0  0.333333  0.377358  0.331050  0.0      0.0  0.694656    0.0   \n",
       "5  0.583333  1.0  1.000000  0.547170  0.337900  0.0      0.0  0.129771    1.0   \n",
       "6  0.645833  0.0  1.000000  0.603774  0.408676  0.0      1.0  0.687023    0.0   \n",
       "7  0.750000  0.0  0.666667  0.433962  0.664384  1.0      1.0  0.656489    0.0   \n",
       "8  0.291667  0.0  0.666667  0.264151  0.198630  0.0      0.0  0.717557    0.0   \n",
       "9  0.666667  1.0  1.000000  0.415094  0.091324  0.0      1.0  0.412214    1.0   \n",
       "\n",
       "    oldpeak  slope        ca  thal  \n",
       "0  0.714286    0.5  1.000000   1.0  \n",
       "1  0.000000    0.0  0.666667   1.0  \n",
       "2  0.464286    0.5  0.000000   1.0  \n",
       "3  0.000000    0.0  0.666667   0.0  \n",
       "4  0.000000    0.5  0.000000   0.0  \n",
       "5  0.214286    0.5  0.333333   1.0  \n",
       "6  0.000000    0.0  0.000000   0.0  \n",
       "7  0.142857    0.0  0.333333   0.0  \n",
       "8  0.035714    0.5  0.000000   0.0  \n",
       "9  0.642857    0.5  0.333333   0.0  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>268.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>246.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>43.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>282.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>221.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>54.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>58.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>318.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>63.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>46.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>249.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  sex   cp  trestbps   chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "0  41.0  0.0  3.0     112.0  268.0  0.0      2.0    172.0    1.0      0.0   \n",
       "1  64.0  1.0  4.0     120.0  246.0  0.0      2.0     96.0    1.0      2.2   \n",
       "2  43.0  1.0  4.0     120.0  177.0  0.0      2.0    120.0    1.0      2.5   \n",
       "3  35.0  1.0  4.0     126.0  282.0  0.0      2.0    156.0    1.0      0.0   \n",
       "4  56.0  1.0  2.0     130.0  221.0  0.0      2.0    163.0    0.0      0.0   \n",
       "5  54.0  1.0  3.0     125.0  273.0  0.0      2.0    152.0    0.0      0.5   \n",
       "6  58.0  1.0  4.0     114.0  318.0  0.0      1.0    140.0    0.0      4.4   \n",
       "7  63.0  0.0  3.0     135.0  252.0  0.0      2.0    172.0    0.0      0.0   \n",
       "8  41.0  0.0  2.0     105.0  198.0  0.0      0.0    168.0    0.0      0.0   \n",
       "9  46.0  1.0  4.0     120.0  249.0  0.0      2.0    144.0    0.0      0.8   \n",
       "\n",
       "   slope   ca  thal  \n",
       "0    1.0  0.0   3.0  \n",
       "1    3.0  1.0   3.0  \n",
       "2    2.0  0.0   7.0  \n",
       "3    1.0  0.0   7.0  \n",
       "4    1.0  0.0   7.0  \n",
       "5    3.0  1.0   3.0  \n",
       "6    3.0  3.0   6.0  \n",
       "7    1.0  0.0   3.0  \n",
       "8    1.0  1.0   3.0  \n",
       "9    1.0  0.0   7.0  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_copy.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.169811</td>\n",
       "      <td>0.324201</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.770992</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.729167</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.273973</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.190840</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.291667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.116438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.374046</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.446429</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.125000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.301887</td>\n",
       "      <td>0.356164</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.648855</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.562500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.339623</td>\n",
       "      <td>0.216895</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.702290</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.520833</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.292453</td>\n",
       "      <td>0.335616</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.618321</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.089286</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.604167</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.188679</td>\n",
       "      <td>0.438356</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.526718</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.386792</td>\n",
       "      <td>0.287671</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.770992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.103774</td>\n",
       "      <td>0.164384</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.740458</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.354167</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.280822</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.557252</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age  sex        cp  trestbps      chol  fbs  restecg   thalach  exang  \\\n",
       "0  0.250000  0.0  0.666667  0.169811  0.324201  0.0      1.0  0.770992    1.0   \n",
       "1  0.729167  1.0  1.000000  0.245283  0.273973  0.0      1.0  0.190840    1.0   \n",
       "2  0.291667  1.0  1.000000  0.245283  0.116438  0.0      1.0  0.374046    1.0   \n",
       "3  0.125000  1.0  1.000000  0.301887  0.356164  0.0      1.0  0.648855    1.0   \n",
       "4  0.562500  1.0  0.333333  0.339623  0.216895  0.0      1.0  0.702290    0.0   \n",
       "5  0.520833  1.0  0.666667  0.292453  0.335616  0.0      1.0  0.618321    0.0   \n",
       "6  0.604167  1.0  1.000000  0.188679  0.438356  0.0      0.5  0.526718    0.0   \n",
       "7  0.708333  0.0  0.666667  0.386792  0.287671  0.0      1.0  0.770992    0.0   \n",
       "8  0.250000  0.0  0.333333  0.103774  0.164384  0.0      0.0  0.740458    0.0   \n",
       "9  0.354167  1.0  1.000000  0.245283  0.280822  0.0      1.0  0.557252    0.0   \n",
       "\n",
       "    oldpeak  slope        ca  thal  \n",
       "0  0.000000    0.0  0.000000  0.00  \n",
       "1  0.392857    1.0  0.333333  0.00  \n",
       "2  0.446429    0.5  0.000000  1.00  \n",
       "3  0.000000    0.0  0.000000  1.00  \n",
       "4  0.000000    0.0  0.000000  1.00  \n",
       "5  0.089286    1.0  0.333333  0.00  \n",
       "6  0.785714    1.0  1.000000  0.75  \n",
       "7  0.000000    0.0  0.000000  0.00  \n",
       "8  0.000000    0.0  0.333333  0.00  \n",
       "9  0.142857    0.0  0.000000  1.00  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build the `Neural Network` model on the train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 2s 94ms/step - loss: 0.6833 - accuracy: 0.6296 - precision: 0.6607 - recall: 0.4205 - val_loss: 0.6635 - val_accuracy: 0.7500 - val_precision: 0.6800 - val_recall: 0.8095\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.6436 - accuracy: 0.8148 - precision: 0.8272 - recall: 0.7614 - val_loss: 0.6346 - val_accuracy: 0.7708 - val_precision: 0.6923 - val_recall: 0.8571\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.6057 - accuracy: 0.8254 - precision: 0.8161 - recall: 0.8068 - val_loss: 0.6026 - val_accuracy: 0.7500 - val_precision: 0.6552 - val_recall: 0.9048\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.5558 - accuracy: 0.8466 - precision: 0.8391 - recall: 0.8295 - val_loss: 0.5579 - val_accuracy: 0.7917 - val_precision: 0.7037 - val_recall: 0.9048\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.5059 - accuracy: 0.8360 - precision: 0.8434 - recall: 0.7955 - val_loss: 0.5103 - val_accuracy: 0.8125 - val_precision: 0.7308 - val_recall: 0.9048\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.4523 - accuracy: 0.8307 - precision: 0.8256 - recall: 0.8068 - val_loss: 0.4732 - val_accuracy: 0.8125 - val_precision: 0.7308 - val_recall: 0.9048\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.4120 - accuracy: 0.8360 - precision: 0.8276 - recall: 0.8182 - val_loss: 0.4383 - val_accuracy: 0.8125 - val_precision: 0.7308 - val_recall: 0.9048\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.3746 - accuracy: 0.8413 - precision: 0.8372 - recall: 0.8182 - val_loss: 0.4463 - val_accuracy: 0.7917 - val_precision: 0.7037 - val_recall: 0.9048\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.3610 - accuracy: 0.8413 - precision: 0.8372 - recall: 0.8182 - val_loss: 0.4288 - val_accuracy: 0.8125 - val_precision: 0.7308 - val_recall: 0.9048\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.3423 - accuracy: 0.8624 - precision: 0.8690 - recall: 0.8295 - val_loss: 0.4548 - val_accuracy: 0.7917 - val_precision: 0.7037 - val_recall: 0.9048\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.3311 - accuracy: 0.8730 - precision: 0.8902 - recall: 0.8295 - val_loss: 0.4561 - val_accuracy: 0.7917 - val_precision: 0.7037 - val_recall: 0.9048\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.3261 - accuracy: 0.8730 - precision: 0.8902 - recall: 0.8295 - val_loss: 0.4604 - val_accuracy: 0.7917 - val_precision: 0.7037 - val_recall: 0.9048\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.3166 - accuracy: 0.8783 - precision: 0.8824 - recall: 0.8523 - val_loss: 0.4818 - val_accuracy: 0.7917 - val_precision: 0.7037 - val_recall: 0.9048\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.3026 - accuracy: 0.8783 - precision: 0.8824 - recall: 0.8523 - val_loss: 0.4622 - val_accuracy: 0.7917 - val_precision: 0.7037 - val_recall: 0.9048\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.3034 - accuracy: 0.8783 - precision: 0.9012 - recall: 0.8295 - val_loss: 0.4776 - val_accuracy: 0.7917 - val_precision: 0.7037 - val_recall: 0.9048\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.2953 - accuracy: 0.8730 - precision: 0.9000 - recall: 0.8182 - val_loss: 0.4729 - val_accuracy: 0.7708 - val_precision: 0.6923 - val_recall: 0.8571\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.2913 - accuracy: 0.8783 - precision: 0.8824 - recall: 0.8523 - val_loss: 0.4964 - val_accuracy: 0.7917 - val_precision: 0.7037 - val_recall: 0.9048\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.2814 - accuracy: 0.8942 - precision: 0.8953 - recall: 0.8750 - val_loss: 0.4868 - val_accuracy: 0.7917 - val_precision: 0.7037 - val_recall: 0.9048\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.2755 - accuracy: 0.8836 - precision: 0.9024 - recall: 0.8409 - val_loss: 0.4818 - val_accuracy: 0.7708 - val_precision: 0.6923 - val_recall: 0.8571\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.2740 - accuracy: 0.8889 - precision: 0.9136 - recall: 0.8409 - val_loss: 0.5051 - val_accuracy: 0.7708 - val_precision: 0.6923 - val_recall: 0.8571\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.2725 - accuracy: 0.8783 - precision: 0.8571 - recall: 0.8864 - val_loss: 0.5167 - val_accuracy: 0.7917 - val_precision: 0.7037 - val_recall: 0.9048\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.2586 - accuracy: 0.8942 - precision: 0.9146 - recall: 0.8523 - val_loss: 0.4781 - val_accuracy: 0.7292 - val_precision: 0.6667 - val_recall: 0.7619\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.2548 - accuracy: 0.9048 - precision: 0.9375 - recall: 0.8523 - val_loss: 0.4922 - val_accuracy: 0.7500 - val_precision: 0.6800 - val_recall: 0.8095\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.2487 - accuracy: 0.9101 - precision: 0.9277 - recall: 0.8750 - val_loss: 0.5034 - val_accuracy: 0.7500 - val_precision: 0.6800 - val_recall: 0.8095\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.2538 - accuracy: 0.8889 - precision: 0.8681 - recall: 0.8977 - val_loss: 0.5149 - val_accuracy: 0.7500 - val_precision: 0.6800 - val_recall: 0.8095\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.2326 - accuracy: 0.9206 - precision: 0.9506 - recall: 0.8750 - val_loss: 0.4960 - val_accuracy: 0.7708 - val_precision: 0.7500 - val_recall: 0.7143\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.2476 - accuracy: 0.9101 - precision: 0.9494 - recall: 0.8523 - val_loss: 0.5268 - val_accuracy: 0.7292 - val_precision: 0.6667 - val_recall: 0.7619\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.2285 - accuracy: 0.9101 - precision: 0.9176 - recall: 0.8864 - val_loss: 0.5130 - val_accuracy: 0.7292 - val_precision: 0.6667 - val_recall: 0.7619\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.2245 - accuracy: 0.9048 - precision: 0.9487 - recall: 0.8409 - val_loss: 0.5181 - val_accuracy: 0.7292 - val_precision: 0.6667 - val_recall: 0.7619\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.4231 - accuracy: 0.8000 - precision: 0.8077 - recall: 0.7500\n",
      "\u001b[1m› \u001b[4mTest Performance ...\u001b[0m\n",
      "    Loss: 0.4230513572692871, Accuracy: 0.800000011920929, Precision: 0.807692289352417, Recall: 0.75\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=[13]),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[\n",
    "      'accuracy',\n",
    "      tf.keras.metrics.Precision(),\n",
    "      tf.keras.metrics.Recall()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    x_train, \n",
    "    y_train, \n",
    "    epochs=100, \n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True)])\n",
    "\n",
    "# Evaluate the model on the testing set\n",
    "test_metrics = model.evaluate(x_test, y_test)\n",
    "\n",
    "# Print the testing accuracy\n",
    "styled_print(\"Test Performance ...\", header=True)\n",
    "styled_print(\n",
    "    f\"Loss: {test_metrics[0]}, Accuracy: {test_metrics[1]}, Precision: {test_metrics[2]}, Recall: {test_metrics[3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 3ms/step\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "No Heart Disease       0.79      0.84      0.82        32\n",
      "   Heart Disease       0.81      0.75      0.78        28\n",
      "\n",
      "        accuracy                           0.80        60\n",
      "       macro avg       0.80      0.80      0.80        60\n",
      "    weighted avg       0.80      0.80      0.80        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions on the test set\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred = (y_pred >= 0.5).astype(int)\n",
    "\n",
    "# Generate a classification report\n",
    "target_names = ['No Heart Disease', 'Heart Disease']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build the `SVM` model on the train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the SVM classifier\n",
    "svm = SVC(kernel='linear')\n",
    "\n",
    "# Train the SVM classifier\n",
    "svm.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy: 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions on the test set\n",
    "y_pred = svm.predict(x_test)\n",
    "\n",
    "# Print the testing accuracy\n",
    "print('Testing accuracy:', svm.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "No Heart Disease       0.81      0.91      0.85        32\n",
      "   Heart Disease       0.88      0.75      0.81        28\n",
      "\n",
      "        accuracy                           0.83        60\n",
      "       macro avg       0.84      0.83      0.83        60\n",
      "    weighted avg       0.84      0.83      0.83        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate a classification report\n",
    "target_names = ['No Heart Disease', 'Heart Disease']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After trying and testing Neural Network and SVM Classifier, we found that both results in almost similar performance. As there couldn't be many variations possible in Neural Networks, as a further extension of this code we can try different hyperparameters for Neural Networks to boos the performance of the model. We are keeping it outside the scope of assignment but it is very straight forward in our setup. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will follow these steps to create and test the required models: \n",
    "- Data Cleaning and Preprocessing\n",
    "    - Put the text data into a dataframe.\n",
    "    - Clean the text data by \n",
    "        - Removing stop words, \n",
    "        - Removing Numbers, \n",
    "        - Removing Punctuation\n",
    "        - Lemmatizing each word\n",
    "        - Converting to lower case\n",
    "- Feature Engineering\n",
    "    - Vectorization\n",
    "        - Convert each observation into a vector which is of length $n$ where $n$ is the amount of unique words in our dataset. \n",
    "        - Each element of the vector will correspond to each unique word in the dataset, showing the frequency of said word in the given observation. \n",
    "- Model Creation\n",
    "    - Split the dataset into train and test (80:20)\n",
    "    - Create Models (Random Forest, Naive Bays, Decision Tree)\n",
    "    - Performance Evaluation of Each Classifiers.\n",
    "- Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This the second set of strap locks that I've o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>First of all I want to say I love a tube amp d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i only bought with the idea that a \"FULL\" vers...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you're like me, you probably bought this to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Didn't know what to expect for under $10, but ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Score\n",
       "0  This the second set of strap locks that I've o...      1\n",
       "1  First of all I want to say I love a tube amp d...      1\n",
       "2  i only bought with the idea that a \"FULL\" vers...      0\n",
       "3  If you're like me, you probably bought this to...      1\n",
       "4  Didn't know what to expect for under $10, but ...      1"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"./data/musical.tsv\", sep=\"\\t\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-50-06c7a3887b3e>:11: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data[\"Review\"] = data[\"Review\"].str.replace('[^a-z\\s]', '')\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Stop Words and Lemmatizer\n",
    "en_stops = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# Remove stop words\n",
    "data[\"Review\"] = data[\"Review\"].\\\n",
    "    apply(lambda x : ' '.join([word.lower() for word in x.split() if word not in (en_stops)]))\n",
    "\n",
    "# Remove non-alpha or space characters\n",
    "data[\"Review\"] = data[\"Review\"].str.replace('[^a-z\\s]', '')\n",
    "\n",
    "# Lemmatize words\n",
    "data[\"Review\"] = data[\"Review\"].apply(lambda x : ' '.join([lemmatizer.lemmatize(x) for x in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this second set strap lock ive owned they litt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>first i want say i love tube amp distortion ov...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bought idea full version behringers sequence p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>if like me probably bought hook xlr microphone...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>didnt know expect proved worth gamblethis cabl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Score\n",
       "0  this second set strap lock ive owned they litt...      1\n",
       "1  first i want say i love tube amp distortion ov...      1\n",
       "2  bought idea full version behringers sequence p...      0\n",
       "3  if like me probably bought hook xlr microphone...      1\n",
       "4  didnt know expect proved worth gamblethis cabl...      1"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = data[\"Review\"]\n",
    "vec = CountVectorizer()\n",
    "bow = vec.fit_transform(data[\"Review\"])\n",
    "bow = pd.DataFrame(bow.toarray(), columns=vec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ab</th>\n",
       "      <th>abcd</th>\n",
       "      <th>abelton</th>\n",
       "      <th>abide</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>ableto</th>\n",
       "      <th>ableton</th>\n",
       "      <th>abnormality</th>\n",
       "      <th>about</th>\n",
       "      <th>...</th>\n",
       "      <th>zero</th>\n",
       "      <th>zi</th>\n",
       "      <th>zip</th>\n",
       "      <th>ziplock</th>\n",
       "      <th>zipper</th>\n",
       "      <th>zipping</th>\n",
       "      <th>ziptie</th>\n",
       "      <th>znet</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7147 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ab  abcd  abelton  abide  ability  able  ableto  ableton  abnormality  \\\n",
       "0   0     0        0      0        0     0       0        0            0   \n",
       "1   0     0        0      0        0     0       0        0            0   \n",
       "2   0     0        0      0        0     0       0        0            0   \n",
       "3   0     0        0      0        0     0       0        0            0   \n",
       "4   0     0        0      0        0     0       0        0            0   \n",
       "\n",
       "   about  ...  zero  zi  zip  ziplock  zipper  zipping  ziptie  znet  zone  \\\n",
       "0      0  ...     0   0    0        0       0        0       0     0     0   \n",
       "1      0  ...     0   0    0        0       0        0       0     0     0   \n",
       "2      0  ...     0   0    0        0       0        0       0     0     0   \n",
       "3      0  ...     0   0    0        0       0        0       0     0     0   \n",
       "4      0  ...     0   0    0        0       0        0       0     0     0   \n",
       "\n",
       "   zoom  \n",
       "0     0  \n",
       "1     0  \n",
       "2     0  \n",
       "3     0  \n",
       "4     0  \n",
       "\n",
       "[5 rows x 7147 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = pd.concat([data[\"Score\"], bow], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>ab</th>\n",
       "      <th>abcd</th>\n",
       "      <th>abelton</th>\n",
       "      <th>abide</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>ableto</th>\n",
       "      <th>ableton</th>\n",
       "      <th>abnormality</th>\n",
       "      <th>...</th>\n",
       "      <th>zero</th>\n",
       "      <th>zi</th>\n",
       "      <th>zip</th>\n",
       "      <th>ziplock</th>\n",
       "      <th>zipper</th>\n",
       "      <th>zipping</th>\n",
       "      <th>ziptie</th>\n",
       "      <th>znet</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7148 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Score  ab  abcd  abelton  abide  ability  able  ableto  ableton  \\\n",
       "0      1   0     0        0      0        0     0       0        0   \n",
       "1      1   0     0        0      0        0     0       0        0   \n",
       "2      0   0     0        0      0        0     0       0        0   \n",
       "3      1   0     0        0      0        0     0       0        0   \n",
       "4      1   0     0        0      0        0     0       0        0   \n",
       "\n",
       "   abnormality  ...  zero  zi  zip  ziplock  zipper  zipping  ziptie  znet  \\\n",
       "0            0  ...     0   0    0        0       0        0       0     0   \n",
       "1            0  ...     0   0    0        0       0        0       0     0   \n",
       "2            0  ...     0   0    0        0       0        0       0     0   \n",
       "3            0  ...     0   0    0        0       0        0       0     0   \n",
       "4            0  ...     0   0    0        0       0        0       0     0   \n",
       "\n",
       "   zone  zoom  \n",
       "0     0     0  \n",
       "1     0     0  \n",
       "2     0     0  \n",
       "3     0     0  \n",
       "4     0     0  \n",
       "\n",
       "[5 rows x 7148 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = processed_data.loc[:, processed_data.columns != \"Score\" ]\n",
    "y = processed_data[\"Score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial Naive bayes is a classification algorithm which will classify an input as the class with the most likely posterior where bayesian probability formula is $$p(L_k)=\\frac{p(L_k)*p(W|L_k)}{P(W)}$$\n",
    "\n",
    "where \n",
    "- L is the label of the data(score in this case), \n",
    "- W is an observation, and \n",
    "- k is relative to the kth observation. \n",
    "\n",
    "This can also be thought of as $$posterior=prior*\\frac{likelihood}{evidence}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(x_train, y_train)\n",
    "nb_preds = nb_clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.75      0.81      0.78        86\n",
      "     Class 1       0.85      0.80      0.82       114\n",
      "\n",
      "    accuracy                           0.81       200\n",
      "   macro avg       0.80      0.81      0.80       200\n",
      "weighted avg       0.81      0.81      0.81       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate a classification report\n",
    "target_names = ['Class 0', 'Class 1']\n",
    "print(classification_report(y_test, nb_preds, target_names=target_names))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Best score: 0.65375\n"
     ]
    }
   ],
   "source": [
    "# Create a decision tree classifier object\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "\n",
    "# Define the hyperparameter grid for the decision tree\n",
    "params = {\n",
    "    'max_depth': [4, 5, 6, 7, 8, 9, 10, None],\n",
    "    'min_samples_split': [2, 3, 4],\n",
    "    'min_samples_leaf': [1, 2, 3, 4, 5]\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object and fit it to the training data\n",
    "grid_search = GridSearchCV(estimator=dt_clf, param_grid=params, cv=5)\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding mean cross-validated score\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.58      0.65      0.61        86\n",
      "     Class 1       0.71      0.64      0.67       114\n",
      "\n",
      "    accuracy                           0.65       200\n",
      "   macro avg       0.64      0.65      0.64       200\n",
      "weighted avg       0.65      0.65      0.65       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the best model from the grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict on the test set using the best model\n",
    "y_pred = best_model.predict(x_test)\n",
    "\n",
    "# Generate a classification report\n",
    "target_names = ['Class 0', 'Class 1']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that even after extensive Grid Search, the performanc of Decision Tree model is not upto the mark. Let's try Random Forest model and perform model analysis. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "Best score: 0.78\n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameters to tune\n",
    "# params = {\n",
    "#     'n_estimators': [50, 100, 200, 'warn', 'raise'],\n",
    "#     'max_depth': [3, 5, None],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "#     'min_samples_leaf': [1, 2, 4],\n",
    "#     'criterion': ['gini', 'entropy'],\n",
    "#     'max_features': ['auto', 'sqrt', 'log2', None]\n",
    "# }\n",
    "params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "}\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "rfc = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(rfc, params, cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit the GridSearchCV object to the data\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding mean cross-validated score\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.81      0.73      0.77        86\n",
      "     Class 1       0.81      0.87      0.84       114\n",
      "\n",
      "    accuracy                           0.81       200\n",
      "   macro avg       0.81      0.80      0.80       200\n",
      "weighted avg       0.81      0.81      0.81       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the best model from the grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict on the test set using the best model\n",
    "y_pred = best_model.predict(x_test)\n",
    "\n",
    "# Generate a classification report\n",
    "target_names = ['Class 0', 'Class 1']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can se from three classification reports, the performance comparison looks like following\n",
    "\n",
    "| Model | Accuracy | F1 | Precision | Recall |\n",
    "| ----- | -------- | -- | --------- | ------ |\n",
    "| Naive Bayes | 0.81 | 0.82 | 0.85 | 0.80 |\n",
    "| Decision Tree | 0.65 | 0.67 | 0.71 | 0.63 |\n",
    "| Random Forest | 0.81 | 0.84 | 0.81 | 0.87 |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see overall Random Forest is better in performance compared to Naive Bayes and Decision Tree. We believe that with further extensive Grid, we can further improve the performance of Random Forest."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
